{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gabo RAG | Daniel Felipe Montenegro\n",
        "**'Gabo'** is a **RAG (Retrieval-Augmented Generation)** system designed to enhance the capabilities of **LLMs (Large Language Models)** such as **'Llama 3.1'** or **'Phi 3.5**'. This project honors Colombian author **Gabriel García Márquez** by marking the tenth anniversary of his death, creating a specialized assistant to answer questions about his work, and using new technologies to further reveal his literary legacy.\n",
        "\n",
        "[**Python Notebook**](https://github.com/dafmontenegro/gabo-rag/blob/master/gabo_rag.ipynb) | [**Webpage**](https://dafmontenegro.com/gabo-rag/) | [**Repository**](https://github.com/dafmontenegro/gabo-rag)\n",
        "\n",
        "- [1. Tools and Technologies](#1-tools-and-technologies)\n",
        "- [2. How to run Ollama in Google Colab?](#2-how-to-run-ollama-in-google-colab)\n",
        "    - [2.1 Ollama Installation](#21-ollama-installation)\n",
        "    - [2.2 Run 'ollama serve'](#22-run-ollama-serve)\n",
        "    - [2.3 Run 'ollama pull \\<model\\_name\\>'](#23-run-ollama-pull-model_name)\n",
        "- [3. Exploring LLMs](#3-exploring-llms)\n",
        "- [4. Data Extraction and Preparation](#4-data-extraction-and-preparation)\n",
        "    - [4.1 Web Scraping and Chunking](#41-web-scraping-and-chunking)\n",
        "    - [4.2 Embedding Model: Nomic](#42-embedding-model-nomic)\n",
        "- [5. Storing in the Vector Database](#5-storing-in-the-vector-database)\n",
        "    - [5.1 Making Chroma Persistent](#51-making-chroma-persistent)\n",
        "    - [5.2 Adding Documents to Chroma](#52-adding-documents-to-chroma)\n",
        "- [6. Use a Vectorstore as a Retriever](#6-use-a-vectorstore-as-a-retriever)\n",
        "- [7. RAG (Retrieval-Augmented Generation)](#7-rag-retrieval-augmented-generation)\n",
        "- [8. References](#8-references)\n",
        "\n",
        "## Author\n",
        "\n",
        "- **Daniel Felipe Montenegro** [GitHub](https://github.com/dafmontenegro) | [Blog](https://www.youtube.com/@MiAmigoMelquiades) | [X](https://x.com/dafmontenegro)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Tools and Technologies\n",
        "\n",
        "- [**Ollama**](https://ollama.com/): Running models ([Llama 3.1](https://ollama.com/library/llama3.1) or [Phi 3.5](https://ollama.com/library/phi3.5)) and embeddings ([Nomic](https://ollama.com/library/nomic-embed-text))\n",
        "- [**LangChain**](https://python.langchain.com/docs/introduction/): Framework and web scraping tool\n",
        "- [**Chroma**](https://docs.trychroma.com/): Vector database\n",
        "\n",
        "> A special thanks to ['Ciudad Seva (Casa digital del escritor Luis López Nieves)'](https://ciudadseva.com/quienes-somos/), from which the texts used in this project were extracted and where a comprehensive [Spanish Digital Library](https://ciudadseva.com/biblioteca/) is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. How to run Ollama in Google Colab?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Ollama Installation\n",
        "For this, we simply go to the [Ollama downloads page](https://ollama.com/download/linux) and select **Linux**. The command is as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwzxaz9WN8Sr",
        "outputId": "3652cec4-0063-4038-ce16-521349ac35b9"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Run 'ollama serve'\n",
        "If you run ollama serve, you will encounter the issue where you cannot execute subsequent cells and your script will remain stuck in that cell indefinitely. To resolve this, you simply need to run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BQGW527tO15z"
      },
      "outputs": [],
      "source": [
        "!nohup ollama serve > ollama_serve.log 2>&1 &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running this command, it is advisable to wait a reasonable amount of time for it to execute before running the next command, so you can add something like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YVaKWZCMO3b6"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Run 'ollama pull <model_name>'\n",
        "For this project we will use [Phi-3.5-mini](https://ollama.com/library/phi3.5) the lightweight **Microsoft** model with high capabilities. This project is also extensible to [Llama 3.1](https://ollama.com/library/llama3.1), you would only have to pull that other model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXavgAvmO4z1",
        "outputId": "691a15cf-0ef1-4152-ef32-2a8b8e73466f"
      },
      "outputs": [],
      "source": [
        "!ollama pull phi3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploring LLMs\n",
        "Now that we have our LLM, it's time to test them with what will be our control question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f7Lw0Oh6O6QR"
      },
      "outputs": [],
      "source": [
        "test_message = \"¿Cuántos hijos tiene la señora vieja del cuento Algo muy grave va a suceder en este pueblo?\"\n",
        "# EN:\"How many children does the old woman in the story 'Something Very Serious Is Going to Happen in This Town' have?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 'Gabo' will be designed to function in Spanish, as it was Gabriel García Márquez's native language and his literary work is also in this language.\n",
        "\n",
        "The information is found at the beginning of [the story,](https://ciudadseva.com/texto/algo-muy-grave-va-a-suceder-en-este-pueblo/) so we expect it to be something that can be answered if it has the necessary information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "6Fu72N5yPAOS",
        "outputId": "79499be5-6f38-4c81-9433-c5fa464253da"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ES\n",
        "Fragmento inicial de 'Algo muy grave va a suceder en este pueblo' de Gabriel García Márquez.\n",
        "\"Imagínese usted un pueblo muy pequeño donde hay una señora vieja que tiene dos hijos, uno de 17 y una hija de 14... \"\n",
        "\n",
        "EN\n",
        "Initial excerpt from 'Something Very Serious Is Going to Happen in This Town' by Gabriel García Márquez:\n",
        "\"Imagine a very small town where there is an old woman who has two children, a 17-year-old son and a 14-year-old daughter...\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we can invoke the LLM, we need to install LangChain. [1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9jBMTndPEIp",
        "outputId": "d76669e9-b2f6-46d5-daad-b3169b6be78f"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_SDD9Fq6PGId"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms import Ollama\n",
        "\n",
        "llm_phi = Ollama(model=\"phi3.5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Invoke Phi 3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Gp8wJRpjPJY2",
        "outputId": "6cf13505-324c-410f-f2d7-8ed6df9e3b99"
      },
      "outputs": [],
      "source": [
        "llm_phi.invoke(test_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> At this stage, the model is not expected to be able to answer the question correctly, and they might even hallucinate when trying to give an answer. To solve this problem, we will start building our **RAG** in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Extraction and Preparation\n",
        "To collect the information that our **RAG** will use, we will perform **Web Scraping** of the section dedicated to [Gabriel Garcia Marquez](https://ciudadseva.com/autor/gabriel-garcia-marquez/) in the **Ciudad Seva web site**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Web Scraping and Chunking\n",
        "The first step is to install **Beautiful Soup** so that LangChain's **WebBaseLoader** works correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gU3MDwCmPMNF"
      },
      "outputs": [],
      "source": [
        "!pip install -qU beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next step will be to save the list of sources we will extract from the website into a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_iaDs740PNrl"
      },
      "outputs": [],
      "source": [
        "base_urls = [\"https://ciudadseva.com/autor/gabriel-garcia-marquez/cuentos/\",\n",
        "             \"https://ciudadseva.com/autor/gabriel-garcia-marquez/opiniones/\",\n",
        "             \"https://ciudadseva.com/autor/gabriel-garcia-marquez/otrostextos/\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will create a function to collect all the links that lead to the texts. If we look at the HTML structure, we will notice that the information we're looking for is inside an `<article>` element with the class `status-publish`. Then, we simply extract the `href` attributes from the `<li>` elements inside the `<a>` tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXzAV9rqPPD9",
        "outputId": "9bf9c34f-c805-45b7-a8db-2acb6711c562"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "def get_urls(url):\n",
        "    article = WebBaseLoader(url).scrape().find(\"article\", \"status-publish\")\n",
        "    lis = article.find_all(\"li\", \"text-center\")\n",
        "    return [li.find(\"a\").get(\"href\") for li in lis]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how many texts by the writer we can gather."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAPmhZpWPRZt",
        "outputId": "978b79eb-9343-46f7-c9cf-1e98a84b920e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gabo_urls = []\n",
        "\n",
        "for base_url in base_urls:\n",
        "    gabo_urls.extend(get_urls(base_url))\n",
        "\n",
        "len(gabo_urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the URLs of the texts to feed our **RAG**, we just need to perform web scraping directly from the content of the stories. For that, we will build a function that follows a logic very similar to the previous function, which will initially give us the **raw text**, along with the **reference information** about what we are obtaining (the information found in `<header>`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FUI5q2A2Pc7R"
      },
      "outputs": [],
      "source": [
        "def ciudad_seva_loader(url):\n",
        "    article = WebBaseLoader(url).scrape().find(\"article\", \"status-publish\")\n",
        "    title = \" \".join(article.find(\"header\").get_text().split())\n",
        "    article.find(\"header\").decompose()\n",
        "    texts = (\" \".join(article.get_text().split())).split(\". \")\n",
        "    return [f\"Fragmento {i+1}/{len(texts)} de '{title}': '{text}'\" for i, text in enumerate(texts)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are indeed many ways to perform chunking, several of which are discussed in **\"5 Levels of Text Splitting\"** [2]. The most interesting idea for me about how to split texts, and what I believe fits best in this project, is **Semantic Splitting**. So, following that idea, we will ensure that the function divides all the texts by their periods, thus generating **semantic fragments in Spanish**.\n",
        "\n",
        "> Tests were performed on the **Semantic Similarity** [3] offered by **Langchain**, but the results were worse. In this case, there is no need to do something extremely sophisticated, when the simplest and practically obvious solution is the best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Embedding Model: Nomic\n",
        "I ran several tests with different **embedding models**, including **LLama 3.1** and **Phi 3.5**, but it wasn't until I used `nomic-embed-text` that I saw significantly better results. So, this is the embedding model we'll use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sDll7cv2SMk4"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's pull with Ollama from [Nomic's embedding model](https://ollama.com/library/nomic-embed-text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvixQQrFPiwL",
        "outputId": "768e625a-1137-4160-b248-0873d2c33bf8"
      },
      "outputs": [],
      "source": [
        "!ollama pull nomic-embed-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're going to create our model so we can later use it in **Chroma**, our vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zMqnSIXOPjoZ"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "nomic_ollama_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Storing in the Vector Database\n",
        "**Chroma** is our chosen vector database. With the help of our embedding model provided by **Nomic**, we will store all the fragments generated from the texts, so that later we can query them and make them part of our context for each query to the **LLMs**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Making Chroma Persistent\n",
        "Here we have to think **one step ahead in time**, so we assume that chroma is already persistent, which means that it **exists in a directory**. If we don't do this, what will happen every time we run this **Python Notebook**, is that we will add repeated strings over and over again to the vector database. So it is a good practice to **reset Chroma** and in case it does not exist, it will be created and **simply remain empty**. [4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGbgWAwsPlPY",
        "outputId": "75b30d6c-fb7d-4060-b875-6413e72216a2"
      },
      "outputs": [],
      "source": [
        "!pip install -qU chromadb langchain-chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will create a function that will be specifically in charge of resetting the collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qjHokMwlPn7c"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "def reset_collection(collection_name, persist_directory):\n",
        "    Chroma(\n",
        "        collection_name=collection_name,\n",
        "\t\tembedding_function=nomic_ollama_embeddings,\n",
        "\t\tpersist_directory=persist_directory\n",
        "\t).delete_collection()\n",
        "\n",
        "reset_collection(\"gabo_rag\", \"chroma\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Adding Documents to Chroma\n",
        "We may think that it is enough to just pass it all the text and it will store it completely, but that approach is inefficient and contradictory to the idea of RAG; that is why a whole section was dedicated to Chunking before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6pHyfQePp3F",
        "outputId": "1e9fe521-1a9e-4009-d03d-42dc4fddf1a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5908"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count = 0\n",
        "\n",
        "for gabo_url in gabo_urls:\n",
        "    texts = ciudad_seva_loader(gabo_url)\n",
        "    Chroma.from_texts(texts=texts, collection_name=\"gabo_rag\", embedding=nomic_ollama_embeddings, persist_directory=\"chroma\")\n",
        "    count += len(texts)\n",
        "\n",
        "count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's verify that all fragments were saved correctly in Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYluu0-ZPseN",
        "outputId": "31088fd0-8aae-4e01-f5de-aac9ee72f9dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5908"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store = Chroma(collection_name=\"gabo_rag\", embedding_function=nomic_ollama_embeddings, persist_directory=\"chroma\")\n",
        "\n",
        "len(vector_store.get()[\"ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Here we are accessing the persistent data, not the in-memory data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Use a Vectorstore as a Retriever\n",
        "A retriever is an **interface** that specializes in retrieving information from an **unstructured query**. Let's test the work we did, we will use the same `test_message` as before and see if the retriever can return the **specific fragment** of the text that has the answer (the one quoted in section [3. Exploring LLMs](#3-exploring-llms))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTJV3LclPuSd",
        "outputId": "ed972ce4-f90a-481a-a06f-c1e62abb26dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fragmento 2/40 de 'Algo muy grave va a suceder en este pueblo [Cuento - Texto completo.] Gabriel García Márquez:\n",
            "Imagínese usted un pueblo muy pequeño donde hay una señora vieja que tiene dos hijos, uno de 17 y una hija de 14'\n"
          ]
        }
      ],
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "docs = retriever.invoke(test_message)\n",
        "\n",
        "for doc in docs:\n",
        "    title, article = doc.page_content.split(\"': '\")\n",
        "    print(f\"\\n{title}:\\n{article}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default `Chroma.as_retriever()` will search for the most similar documents and `search_kwargs={”k“: 1}` indicates that we want to limit the output to **1**. [4]\n",
        "\n",
        "> We can see that the document returned to us was the **exact excerpt** that gives the **appropriate context** of our query. So the built retriever is **working correctly.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RAG (Retrieval-Augmented Generation)\n",
        "To better integrate our context to the query, we will make use of a **template** that will help us set up the behavior of the **RAG** and give it indications on how to answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Eres 'Gabo', un asistente especializado en la obra de Gabriel García Márquez. Fuiste creado en conmemoracion del decimo aniversario de su muerte.\n",
        "Responde de manera concisa, precisa y relevante a la pregunta que se te ha hecho, sin desviarte del tema y limitando tu respuesta a un parrafo.\n",
        "Cada consulta que recibas puede estar acompañada de un contexto que corresponde a fragmentos de cuentos, opiniones y otros textos del escritor.\n",
        "\n",
        "Contexto: {context}\n",
        "\n",
        "Pregunta: {input}\n",
        "\n",
        "Respuesta:\n",
        "\"\"\"\n",
        "\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**LangChain** tells us how to use `create_stuff_documents_chain()` to integrate **Phi 3.5** and our **custom prompt**. Then we just need to use `create_retrieval_chain()` to automatically pass to the **LLM** our input along with the context and fill it in the template. [5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yxOFFd0-B5dk"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm_phi, custom_rag_prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's test with our first control question, which allows us to check if the **LLM** is aware of his or her **new identity.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "M7W_Vu8CP-vU",
        "outputId": "d09c97e7-0bdc-4f5f-e6bc-3d20dd7bd5c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ANSWER: Gabo es mi nombre, un asistente diseñado para proporcionar información sobre el ilustre escritor colombiano Gabriel García Márquez y su extensa obra literaria. Mis respuestas están informadas por textos como los fragmentos del cuento \"En este pueblo no hay ladrones\", donde la simplicidad cotidiana refleja las profundidades que el maestro de Macondo exploró en sus narrativas ricas y complejas.\n",
            "\n",
            "CONTEXT: Fragmento 457/714 de 'En este pueblo no hay ladrones [Cuento - Texto completo.] Gabriel García Márquez': 'Comieron sin hablar'\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke({\"input\": \"Hablame de quien eres\"})\n",
        "\n",
        "print(f\"\\nANSWER: {response['answer']}\\nCONTEXT: {response['context'][0].page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally let's conclude with the question that **started all this**...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8q7XXiewQCeg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ANSWER: La señora vieja del cuento 'Algo muy grave va a suceder en este pueblo' posee dos hijos. Uno de los cuales tiene 17 años y la otra, una niña, es de 14 años. Está representando el estilo realista mágico característico que García Márquez utiliza para tejer personajes complejos dentro del tejido familiar densamente poblado en su narrativa.\n",
            "CONTEXT: Fragmento 2/40 de 'Algo muy grave va a suceder en este pueblo [Cuento - Texto completo.] Gabriel García Márquez': 'Imagínese usted un pueblo muy pequeño donde hay una señora vieja que tiene dos hijos, uno de 17 y una hija de 14'\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke({\"input\": test_message})\n",
        "\n",
        "print(f\"\\nANSWER: {response['answer']}\\nCONTEXT: {response['context'][0].page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. References\n",
        "[1] **Ollama. (s. f.). ollama/docs/tutorials/langchainpy.md at main · ollama/ollama. GitHub.** https://github.com/ollama/ollama/blob/main/docs/tutorials/langchainpy.md\n",
        "\n",
        "[2] **FullStackRetrieval-Com. (s. f.). RetrievalTutorials/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb at main · FullStackRetrieval-com/RetrievalTutorials. GitHub.** https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
        "\n",
        "[3] **How to split text based on semantic similarity | 🦜️🔗 LangChain. (s. f.).** https://python.langchain.com/docs/how_to/semantic-chunker/\n",
        "\n",
        "[4] **Chroma — 🦜🔗 LangChain  documentation. (s. f.).** https://python.langchain.com/v0.2/api_reference/chroma/vectorstores/langchain_chroma.vectorstores.Chroma.html\n",
        "\n",
        "[5] **Build a Retrieval Augmented Generation (RAG) App | 🦜️🔗 LangChain. (s. f.).** https://python.langchain.com/docs/tutorials/rag/\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
